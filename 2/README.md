# Abstract

In this report, we implement a neural network on the FashionMNIST dataset, which consists of 28x28 pixel images with the middle column of pixels removed and stored in a separate file. The goal of this report is to implement a neural network that predicts the removed column of the images. We also examine the impact of different methods on the network's output and accuracy.

# Introduction

Image completion is a key task in computer vision that involves predicting missing pixel values in an image. In this exercise, we aim to implement an MLP neural network to predict the missing column in images. We will also explore different methods in neural networks and evaluate their impact on overall accuracy on test data.

![Image with middle column removed](7.jpg)  
*Figure: Image with middle column removed*

# Methods

## MLP Implementation

To start working on the data, we built a neural network with 5 layers:
- Input: 784 inputs (equal to the number of pixels in each image)
- Layer 1: 256 neurons
- Layer 2: 128 neurons
- Layer 3: 64 neurons
- Layer 4: 28 neurons

```python
Net(
  (fc1): Linear(in_features=784, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=28, bias=True)
)
```

The activation function used is RELU. For error calculation, MSE (Mean Squared Error) was used, which is suitable for regression problems. For training, we used SGD (Stochastic Gradient Descent) with a learning rate of 0.001. Data was fed in batches of 32 and updated in each epoch. The loss values in different batches are as follows:
```python
epoch:1, batch: 1,  loss: 0.3032597601413727

epoch:5, batch: 1,  loss: 0.0364769771695137

epoch:7, batch: 1,  loss: 0.03293081000447273

epoch:10, batch: 1500,  loss: 0.03313985466957092
```
Using the MAE (Mean Absolute Error) formula, the average absolute error on the test data was obtained as MAE: 0.3559.

Finally, we present several images for comparison, which include two sections: the first section contains the real image with the actual missing column, and the second section contains the column produced by the neural network.

#### Image 1

![Real Image 1](1.jpg)
*Figure 1: Real Image 1*

![Generated Image 1](2.jpg)
*Figure 2: Image generated by the neural network 1*

#### Image 2

![Real Image 2](3.jpg)
*Figure 3: Real Image 2*

![Generated Image 2](4.jpg)
*Figure 4: Image generated by the neural network 2*

#### Image 3

![Real Image 3](5.jpg)
*Figure 5: Real Image 3*

![Generated Image 3](6.jpg)
*Figure 6: Image generated by the neural network 3*



### Batch Normalization Method

Batch normalization is a technique used to normalize the input to different layers of a neural network before the activation function is applied. This process reduces the internal covariate shift, which occurs when the distribution of input data changes during training, potentially slowing down training and making it harder to converge to an optimal solution.

To evaluate the results, we used the neural network from the previous section and trained it with batch normalization applied. The following results were obtained:

#### Neural Network Architecture with Batch Normalization

```python
Net_batch_normalization(
  (fc1): Linear(in_features=784, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc3): Linear(in_features=128, out_features=64, bias=True)
  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc4): Linear(in_features=64, out_features=28, bias=True)
)
```
Training Loss

```python
epoch:1, batch: 1,  loss: 0.045578472316265106

epoch:5, batch: 1,  loss: 0.04807651415467262

epoch:7, batch: 1,  loss: 0.029884275048971176

epoch:10, batch: 1500,  loss: 0.03308456391096115
```

The mean absolute error (MAE) achieved was: MAE: 0.3635




### Dropout Method

In the dropout method, we reduce the connections between different layers, so not all nodes are fully connected. For this purpose, we set `dropout=0.5` and applied it to the neural network from the previous section with the following configuration:

#### Neural Network Architecture with Dropout

```python
Net_dropout(
  (fc1): Linear(in_features=784, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=28, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)
```

Training Loss

```python
epoch:1, batch: 1,  loss: 0.28008899092674255

epoch:5, batch: 1,  loss: 0.09379275143146515

epoch:7, batch: 1,  loss: 0.06666451692581177

epoch:10, batch: 1500,  loss: 0.07900126278400421
```

The mean absolute error (MAE) achieved was: MAE: 0.3351




### Evaluation of Different Activation Functions

In the standard setup, we used the ReLU activation function. In this section, we used the Sigmoid activation function and obtained the following results:

#### Neural Network Architecture with Sigmoid Activation

```python
Net_sigmoid(
  (fc1): Linear(in_features=784, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=28, bias=True)
)
```

The results obtained after training for 10 epochs with a batch size of 32 are:

```python
epoch:1, batch: 1,  loss: 0.29355284571647644

epoch:5, batch: 1,  loss: 0.12926886975765228

epoch:7, batch: 1,  loss: 0.11413227021694183

epoch:10, batch: 1500,  loss: 0.10271648317575455
```
The mean absolute error (MAE) achieved was: MAE: 0.3011




### Learning Rate

In the standard setup, a learning rate of `0.001` was used, and the results were as previously shown. Now, we present the results obtained with learning rates of `1` and `0.1`.

#### Neural Network Architecture

```python
Net(
  (fc1): Linear(in_features=784, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=28, bias=True)
)
```
learning rate 1
```python
epoch:1, batch: 1,  loss: 0.3256174623966217

epoch:5, batch: 1,  loss: 0.025008196011185646

epoch:7, batch: 1,  loss: 0.01832905039191246

epoch:10, batch: 1500,  loss: 0.026540756225585938
```
Mean absolute error (MAE): 0.36837



learning rate 0.1

```python
epoch:1, batch: 1,  loss: 0.30491235852241516

epoch:5, batch: 1,  loss: 0.01230357401072979

epoch:7, batch: 1,  loss: 0.012982388027012348

epoch:10, batch: 1500,  loss: 0.012105547823011875
```
Mean absolute error (MAE): 0.3716



